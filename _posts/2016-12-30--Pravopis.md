---
layout: post
title: Tutorial: Psaní I/Y pomocí deep learningu
---

V dnešním tutoriálu si ukážeme, jak dobře se jednoduchým modelem deep learningu
naučit, kde se v češtině píše měkké a tvrdé i. Ukážeme si, jak při řešení
takových úloh přemýšlí postupují informatici - komputační lingvisté, a jak se
dá taková aplikace celkem jednoduše naprogramovat. V textu si ukážeme ukázky
zdrojového kódu

# Jak se to vlastně píše

Na základní škole se učíme poměrně dost pravidel, podle kterých se psaní
měkkého a tvrdého i řídí. Pravidla, které se používají jsou:

* měkkost/tvrdost samohlásky (s tou zákeřností, že slova ta slova k tomu musíme
  aspoň trochu znát, abychom věděli, že správně je _štěstí_ a ne _štěstý_),

* znalost kořene slova (je potřeba si pamatovat vyjmenovaná slova),

* znalost pádové koncovky (je potřeba umět pády a vzory, podle kterých se
  skloňuje),

* shoda podmětu s přísudkem (k tomu musíme umět alespoň základním způsobem umět
  větný rozbor a poznat, co je podmět a co je přísudek),

* vyznat se v tom, kdy pravidla kolidují (píšeme _tácy_, přestože _c_ je měkká
  souhláska, protože skloňujeme podle vzoru _hrad_).

Abychom mohli taková pravidla naprogramovat, museli bychom umět nejdřív udělat
automaticky větný rozbor. Automatický větný rozbor ale obvykle spoléhá na to,
že věty jsou gramaticky správně a pokud nejsou, dělá chyby.

# Trénovací a testovací data



# Jednoduchá řešení

Triviálním řešením řešením by bylo všude nechat měkká i. Tímto přístupem se na
našich testovacích dostaneme na krásných __76.4 %__. V komputační lingvistice
(a umělé inteligenci) se tomu obvykle říká _stanovení baseline_. Aby řešení
nějaké úlohy bylo vůbec nějak zajímavé, je potřeba, aby bylo výrazně lepší, než
nějaké triviální řešení.

Zkusíme se ještě na chvíli u těchto jednoduchých řešení zastavit. Můžeme
například zkusit dávat tvrdé y po tvrdých souhláskách _h_, _ch_ (tedy zase
vlastně _h_), _k_ a _r_. U hlásek _d_, _t_ a _n_ ale nemáme možnost jednoduše
poznat, jestli se mají vyslovovat měkce a má za nimi následovat _i_ nebo tvrdě.

Další pozorování, které můžeme udělat je, že pokud slovo začíná na _vi_/_vy_,
většinou se jedná o předponu _vy_ (Vikingové a lama vikuňa prominou).

|                     |  přesnost |
|:--------------------|----------:|
|všechna měkká        |    76.4 % |
| + tvrdé souhlásky   |    77.5 % |
| + předpona _vy_     |    80.0 % |

S řešením, které se dá napsat na jeden řádek pomocí dvou regulárních výrazů
jsme se dostali na úspěšnost 80.0 %. Kdybychom se nad skupinami hlásek
zamysleli důkladněji a pomohli bychom si důkladnější statistickou analýzou,
zvládli bychom se dostat ještě o několik procent výše.

Místo toho se zastavíme u jiného velmi jednoduchého řešení. Z předchozí části
víme, že máme k dispozici celou Wikipedii jako trénovací data. Můžeme si zkusit
pro každé slovo zaznamenat, kolikrát se vyskytlo s jakým pravopisem. Později,
při testování, použijeme pro každé slovo jeho nejčastější pravopis. Následující
tabulka ukazuje, jaké přesnosti dosáhneme po zpracování určitého množství vět.

| počet přečtených vět  |  přesnost |
|----------------------:|----------:|
|                   500 |    77.9 % |
|                 5 000 |    85.3 % |
|                50 000 |    88.6 % |
|               500 000 |    90.4 % |
|             5 000 000 |    90.8 % |

Vidíme, že s rostoucím množstvím zpracovaného textu roste i úspěšnost modelu.
Není to nic překvapivého. Čím více textu vidíme, tím menší pravděpodobnost je,
že v něm uvidíme nějaká slova, která jsme ještě neviděli.

# Než se pustíme do programování

# Model

Na naši úlohu použijeme obousměrnou rekurentní neuronovou síť. Než se dostaneme
k podrobnostem, jak takový model funguje, rozmyslíme si, co po něm vlastně
chceme. Určitě po něm chceme, aby pracoval s větší úspěšností než 91% - potom
by bylo poměrně zbytečné se s něčím takovým mordovat. Jenže také víme, že okolo
těch 91% bude ležet hranice, na kterou je možné se dostat tak, že si
zapamatujeme, jak se píše většina slov. Abychom se dostali za tuto hranici, je
nutné, aby se naše neuronová síť naučila alespoň základní pravidla shody
podmětu s přísudkem, kde se lidé neobejdou bez větného rozboru.

# Trénování

Rekurentní síť vydává nějaký výstup po přečtení každého z písmen, ale nás zde
zajímají jenom, kde je na vstupu i a máme rozhodnout, jestli je měkké nebo
tvrdé.

Na začátku trénování jsou váhy v síti náhodné. V průběhu učení síť vždy přečte
větu, provede svůj odhad a ten porovnáme s tím, co by měla síť vydat do opravdu
a váhy sítě se drobně upraví tak, aby byly blíže tomu, co by síť měla skutečně
vydat.

Pouze z této základní informace se neuronová síť postupně naučí, reprezentovat
vstupní věty tak, aby snadno mohla rozhodovat. Při trénování se nikdy výslovně
neobjevuje pojem slova, souhlásky a samohlásky. Síti nikdo neříká: "Toto je
podmět, protože je v prvním pádě," nepracuje se slovesným rodem, s pády, čísly
a skloňovacími vzory. Všechno to, k čemu jako lidé potřebuje složitá explicitní
pravidla, je schopna během trénování odvodit ze samotných dat.

# Co se síť naučila

Nevýhodou neuronových sítí je to, že nemáme možnost nějak jednoduše zjistit, co
se vlastně naučila.

Jedním ze způsobů, jak zjistit něco o tom, co se síť naučila, je podívat se,
jak vypadá reprezentace znaků, kterou se síť naučila používat. Znaky
reprezentuje pomocí vektorů s 32 dimenzemi - to se pochopitelně špatně
vizualizuje.
