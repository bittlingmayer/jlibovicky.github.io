---
layout: post
title: Kontrola I/Y pomocí hlubokého učení
---

V dnešním příspěvku si ukážeme, jak dobře se jednoduchým modelem deep learningu
naučit, kde se v češtině píše měkké a tvrdé i. Ukážeme si, jak při řešení
takových úloh přemýšlí postupují informatici — komputační lingvisté, a jak se
dá taková aplikace celkem jednoduše naprogramovat.

# Jak se to vlastně píše

Na základní škole se učíme poměrně dost pravidel, podle kterých se psaní
měkkého a tvrdého i řídí. Pravidla, které se používají jsou:

* měkkost/tvrdost samohlásky (s tou zákeřností, že slova ta slova k tomu musíme
  aspoň trochu znát, abychom věděli, že správně je _štěstí_ a ne _štěstý_),

* znalost kořene slova (je potřeba si pamatovat vyjmenovaná slova),

* znalost pádové koncovky (je potřeba umět pády a vzory, podle kterých se
  skloňuje),

* shoda podmětu s přísudkem (k tomu musíme umět alespoň základním způsobem umět
  větný rozbor a poznat, co je podmět a co je přísudek),

* vyznat se v tom, kdy pravidla kolidují (píšeme _tácy_, přestože _c_ je měkká
  souhláska, protože skloňujeme podle vzoru _hrad_).

Abychom mohli taková pravidla naprogramovat, museli bychom umět nejdřív udělat
automaticky větný rozbor včetně všech pádů, rodů a čísel, abychom mohli všechna
pravidla použít. Pro představu, jak může větný rozbor fungovat, si můžete
vyzkoušet napříkald
[Treex](https://lindat.mff.cuni.cz/services/treex-web/result/2HfygMLDnvZm63ZX4EF).

Automatické větné analyzátory obvykle spoléhají na to, že věty jsou gramaticky
správně a pokud nejsou, dělá chyby. Vzhledem k tomu, že chceme věty opravovat
je to poměrně nepříjemná vlastnost.

# Trénovací a testovací data

Jakékoli strojové učení vždy potřebuje nějaká data, ze kterých se bude učit. V
případě této úlohy můžeme trénovací data získat velmi snadno tak, že stáhneme z
Internetu větší množství českého textu a budeme doufat, že je z většiny
gramaticky správně. Jednoduše dá stáhnout najednou například celá [česká
Wikipedie](https://dumps.wikimedia.org/cswiki/latest/) v XML formátu.

Text z Wikipedie je ještě potřeba trochu upravit a pročistit. Text se dá
například automaticky rozdělit na věty (knihovna NLTK obsahuje i modely pro
češtinu). Pro jednoduchost ještě nahradíme všechny znaky, které nejsou z české
abecedy nějakým speciálním symbolem (třeba `_`) — Wikipedie obsahuje spoustu
útržků textů v jiných abecedách a veškerý text převedeme na malá písmena.

Nyní už zvývá jenom změnit všechna i na měkká a zapamatovat si, kde se mají
změnit na trvdá. Výsledek může vypadat nějak takto:

*Vstup:*

```
aristotelés dále určil poloměr země, kterí ale odhadl na dvojnásobek...

v aristotelovském modelu země stojí a měsíc se sluncem a hvězdami krouží...

mišlenki aristotelovi rozvinul ve 2. století našeho letopočtu klaudios...
```

*Výstup*

```
00001000000000000000000000000000001000000100000000000000000000001000000100000...

02000002000100000000200000100000000000000001000000000000000000000001000000000...

00000000000000000010000000000001000002000000000000000000020000000000000000000...
```

V tomu výstupu jednička značí měkké i, dvojka značí y a nula všechny ostatní
znaky. Můžete si prohlédnout kód pro [rozdělení textu na
věty](/assets/code/yi/sentence_split.py) a [přípravu
dat](/assets/code/yi/format_data.py). V době stahování bylo na české Wikipedie
přibližně 5 milionů českých vět.

Pokud něco řešíme pomocí strojového učení — a je jedno jestli je to dnes tak
populární hluboké učení nebo jiné metody, je potřeba striktně dodělovat
trénovací a testovací data. Trénovací používá k naučení modelu a je nelze se
divit, že na nich bude fungovat poměrně dobře. Když nějaký model vyvíjíme, mělo
by nás spíše zajímat, jak si povede, až uvidí nějaká data, která při trénování
neviděl. To se obykle řeší tak, že si necháme nějaká data stranou, model na
nich netrénujeme a tato data používáme pouze k tomu, abychom porovvali, který
model je lepší a který je horší. Pro naše další experimenty si odložíme stranou
tisíc vět a ty budeme používat pro testování našich metod.

# Jednoduchá řešení

Triviálním řešením řešením by bylo všude nechat měkká i. Tímto přístupem se na
našich testovacích dostaneme na krásných __76.4 %__. V komputační lingvistice
(a umělé inteligenci) se tomu obvykle říká _stanovení baseline_. Aby řešení
nějaké úlohy bylo vůbec nějak zajímavé, je potřeba, aby bylo výrazně lepší, než
nějaké triviální řešení.

Zkusíme se ještě na chvíli u těchto jednoduchých řešení zastavit. Můžeme
například zkusit dávat tvrdé y po tvrdých souhláskách _h_, _ch_ (tedy zase
vlastně _h_), _k_ a _r_. U hlásek _d_, _t_ a _n_ ale nemáme možnost jednoduše
poznat, jestli se mají vyslovovat měkce a má za nimi následovat _i_ nebo tvrdě.

Další pozorování, které můžeme udělat je, že pokud slovo začíná na _vi_/_vy_,
většinou se jedná o předponu _vy_ (Vikingové a lama vikuňa prominou).

|                     |  přesnost |
|:--------------------|----------:|
|všechna měkká        |    70.4 % |
| + tvrdé souhlásky   |    77.5 % |
| + předpona _vy_     |    80.0 % |

S řešením, které se dá napsat na jeden řádek pomocí dvou regulárních výrazů
jsme se dostali na úspěšnost 80.0 %. Kdybychom se nad skupinami hlásek
zamysleli důkladněji a pomohli bychom si důkladnější statistickou analýzou,
zvládli bychom se dostat ještě o několik procent výše.

Místo toho se zastavíme u jiného velmi jednoduchého řešení. Z předchozí části
víme, že máme k dispozici celou Wikipedii jako trénovací data. Můžeme si zkusit
pro každé slovo zaznamenat, kolikrát se vyskytlo s jakým pravopisem. Později,
při testování, použijeme pro každé slovo jeho nejčastější pravopis. Následující
tabulka ukazuje, jaké přesnosti dosáhneme po zpracování určitého množství vět.

| počet přečtených vět  |  přesnost |
|----------------------:|----------:|
|                   500 |    77.9 % |
|                 5 000 |    85.3 % |
|                50 000 |    88.6 % |
|               500 000 |    90.4 % |
|             5 000 000 |    90.8 % |


Vidíme, že s rostoucím množstvím zpracovaného textu roste i úspěšnost modelu.
Není to nic překvapivého. Čím více textu vidíme, tím menší pravděpodobnost je,
že v něm uvidíme nějaká slova, která jsme ještě neviděli.

# Model

Než se dostaneme k podrobnostem, jak takový model funguje, rozmyslíme si, co
po něm vlastně chceme. Určitě po něm chceme, aby pracoval s větší úspěšností
než 91% — potom by bylo poměrně zbytečné se s něčím takovým mordovat. Jenže
také víme, že okolo těch 91% bude ležet hranice, na kterou je možné se dostat
tak, že si zapamatujeme, jak se píše většina slov. Abychom se dostali za tuto
hranici, je nutné, aby se naše neuronová síť naučila alespoň základní pravidla
shody podmětu s přísudkem, kde se lidé neobejdou bez větného rozboru.

Na naši úlohu použijeme obousměrnou rekurentní neuronovou síť. Rekurentní
neuronová síť funguje tak, že pokaždé, když dostane nějaký vstup, provede
update svého vnitřního stavu a vydá nějaký výstup. Schéma kousku sítě rozvinuté
v čase vidíme na následujícím obrázku.

![sequence-labeling](/assets/rnn_cs.svg)

Rekurentní síť dělá v každém kroku (tedy s každým písmenkem) tu samou operaci.
To se může zdát na první pohled zvláštní, ale je to přesně to, co naší síti
chceme. Každé písmeno má jinou naučenou vektoru reprezentaci, a když ji síť
obdrží, rozhodne, co udělá. Vnitřní stav rekurentní sítě je vlastně paměť, kde
si ukládá, jaké vstupy viděla v minulosti a co to znamená pro vstupy, které
mají teprve přijít.

Síť se učí především reprezentovat vstupní písmena a už viděný text takovým
způsobem, aby dokázala svůj úkol. Učení vhodné reprezentace se často zdůrazňuje
jako jedna z nejdůležitějších vlastností hlubokého učení. V textu nijak
nezdůrazňujeme lingvistické koncepty, které nám pomáhají pravopis nějakým
způsobem pojmově uchopit. Je to prostě proud znaků a neuronová síť s ním musí
nějakým způsobem poradit.

Síť ještě vylepšíme jednoduchým trikem. Použijeme nezávisle dvě rekurentní
neuronové sítě. Jedna bude číst text odpředu a bude se snažit odhadnout
pravopis podle toho, co bylo ve větě nalevo od posledního písmene, druhá síť
bude používat to, co bylo napravo od něj.

# Trénování

Rekurentní síť vydává nějaký výstup po přečtení každého z písmen, ale nás zde
zajímají jenom, kde je na vstupu i a máme rozhodnout, jestli je měkké nebo
tvrdé. Výstupy na ostatních místech sítě můžeme ignorovat.

Na začátku trénování jsou váhy v síti náhodné. V průběhu učení síť vždy přečte
větu, provede svůj odhad a ten porovnáme s tím, co by měla síť vydat do opravdu
a váhy sítě se drobně upraví tak, aby byly blíže tomu, co by síť měla skutečně
vydat.

Pouze z této základní informace se neuronová síť postupně naučí, reprezentovat
vstupní věty tak, aby snadno mohla rozhodovat. Při trénování se nikdy výslovně
neobjevuje pojem slova, souhlásky a samohlásky. Síti nikdo neříká: "Toto je
podmět, protože je v prvním pádě," nepracuje se slovesným rodem, s pády, čísly
a skloňovacími vzory. Všechno to, k čemu jako lidé potřebuje složitá explicitní
pravidla, je schopna během trénování odvodit ze samotných dat.

Naše síť trénovaná na ne příliš výkonné grafické kartě zpracovala 5 milionů vět
za 8 hodin a nakonec dosáhla úspěšnosti **98 %**.


|metoda               |  přesnost |
|:--------------------|----------:|
|všechna měkká        |    70.4 % |
|jednoduchá pravidla  |    80.0 % |
|nejčastější pravopis |    90.8 % |
|neuronová síť        |    98.3 % |

# Co se síť naučila

Nevýhodou neuronových sítí je to, že nemáme možnost nějak jednoduše zjistit, co
se vlastně naučila. Jakousi základní představu si můžeme udělat z takzvaných
učících křivek. To je graf, který má na ose *x* počet použitých trénovacích dat
a na ose *y* úspěšnost modelu.

![sequence-labeling](/assets/rnn_learning_curve_cs.svg)

Z grafu na první pohled vidíme, že neuronová síť potřebovala poměrně dost
trénovacích příkladů na to, aby se naučila dělat něco lepšího, než všude dávat
měkké i. Potřeboval k tomu 13 tisíc vět.

Zatímco algoritmus, který si pamatuje nejčastější pravopis každého slova
potřeboval 1 500 vět k tomu, aby byl úspěšnější, než naše jednoduchá pravidla,
neuronová síť k tomu potřebovala 33 000 trénovacích vět - to je 2200 normostran
textu, více než trojnásobek délky Dostojevského Zločinu a trestu. Síť překonala
pamatování si nejčastějšího pravopisu až po 300 000 slovech. Ty by při průměrné
rychlosti čtení 200 slov za minutu trvalo přečíst 17 dní bez přestávky.

Další možností, jak zjistit, co se naše síť naučila, je provést ručně rozbor
chyb na nějakých zajímavých příkladech. My se podíváme na několik vět z
[televizního diktátu z roku
2008](http://zpravy.idnes.cz/umite-pravopis-vyplnte-si-sverakuv-diktat-na-idnes-cz-ppd-/domaci.aspx?c=A080829_111241_studium_bar).

* Děti se jako vždycky nejvíc těšily na slavnou velrybí kostru.

|:-------------|:-------------------------------------------------------------|
|pravidla      |Děti se jako vždicky nejvíc těšili na slavnou velrybí kostru. |
|nejčastější   |Děti se jako vždycky nejvíc těšili na slavnou velrybí kostru. |
|neuronová síť | ... |

* Rodiče zase lákaly archeologické nálezy kostěných nástrojů starých kultur.

|:-------------|:--------------------------------------------------------------------------|
|pravidla      |Rodiče zase lákali archeologické nálezi kostěních nástrojů starých kultur. |
|nejčastější   |Rodiče zase lákaly archeologické nálezy kostěných nástrojů starých kultur. |
|neuronová síť | ... |

* V oddělení nerostů pak byli zaujati třpytivými drahokamy, hýřícími kouzelnými
  barvami.

|:-------------|:--------------------------------------------------------------------------------------|
|pravidla      |V oddělení nerostů pak bili zaujati třpitivími drahokamy, hýřícími kouzelními barvami. |
|nejčastější   |V oddělení nerostů pak byly zaujati třpytivými drahokamy, hýřícími kouzelnými barvami. |
|neuronová síť | ... |

* Mezi plazi vás určitě zaujmou krokodýli.

|:-------------|:----------------------------------------|
|pravidla      |Mezi plazi vás určitě zaujmou krokodíli.
|nejčastější   |Mezi plazy vás určitě zaujmou krokodýli.
|neuronová síť | ... |

Na těchto pár příkladech vidíme, že pravidla, která fungují na 80% dobře stále
produkují text, který vypadá jako by ho psal téměř analfabet. Deset procent,
které od sebe dělí jednoduchá pravidla a používání nejčastějšího pravopisu na
druhou stranu vytváří dojem, že je text téměř bez chyby. Chyby jsou na místech,
kde je skutečně potřeba využít pravidel o gramatické shodě.
